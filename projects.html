<!-- 
  Author: Li Ding
  2020/01/23
-->

<!DOCTYPE html>
<html style="height: 100%;">

<head>
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Li Ding | 丁立</title>
  <meta name="description" content="Li Ding's personal website.">
  <meta name="author" content="Li Ding">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Raleway:400,300,600">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.7.2/css/all.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="images/favicon.png">-->


<body>
  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <header class="container">
  <br>
  <nav class="navbar">
    <ul class="navbar-list">
      <li class="navbar-item">
        <a class="navbar-link" href=".">Li Ding | 丁立</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link active" href="projects.html">Projects</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link" href="publications.html">Publications</a>
      </li>
      <li class="navbar-item">
        <a class="navbar-link" href="misc.html">Misc.</a>
      </li>
    </ul>
  </nav>
</header>

<div class="container">
  <hr>



  <h5>
    <b>Human Cognitive Load Assessment</b>
  </h5>
  <p style="color:dimgray">
    Research Project at MIT<br>
    Supported by Veoneer and MIT AHEAD Consortium<br>
    2018 - present<br>
  </p>

  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <img src="images/proj_cog.jpg" style="width:95%">
    </div>
    <div class="six columns">
      <p>
        How do we measure the state of the human mind? Cognitive load has been
        shown to be an important variable for understanding human performance on
        a variety of tasks including public speaking, education, machine
        operation, and driving. Our research focuses on vision-based non-contact
        cognitive load estimation using machine learning approaches on visual
        features including pupil dynamics, glance behaviors, and facial
        expressions.
      </p>
    </div>
  </div>
  <br>

  <p style="color:dimgray; font-size:125%;">
    <b>References</b>
  </p>

  <p style="margin-top: 1.2em;">
    <b>Pupils and Blinks in the Wild</b>
    <br>
    <b>Li Ding</b>, Jack Terwilliger, Aishni Parab, Meng Wang, Bruce Mehler, Bryan Reimer, and Lex Fridman
    <br>
    [Under Review at ECCV 2020]
  </p>

  <hr>

  <h5>
    <b>Driving Scene Perception and Edge Case Enumeration</b>
  </h5>
  <p style="color:dimgray">
    Research Project at MIT<br>
    Supported by Toyota<br>
    2017 - 2020<br>
  </p>

  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <img src="images/proj_seg.jpg" style="width:95%">
    </div>
    <div class="six columns">
      <p>
        Solving the driving scene perception problem for driver-assistance
        systems and autonomous vehicles requires accurate and robust model
        performance in various driving scenarios, including those
        rarely-occurring edge cases. Aiming at developing a real-time perception
        system prototype, our research involves novel methods for video scene
        segmentation, large-scale driving data collection, semi-automated
        annotation, and edge case enumeration.
      </p>
    </div>
  </div>
  <br>

  <p style="color:dimgray; font-size:125%;">
    <b>References</b>
  </p>

  <p style="margin-top: 1.2em;">
    <b>Semantic Understanding on Semantic Scenes</b>
    <br>
    <b>Li Ding</b>, Meng Wang, Rini Sherony, Bruce Mehler, and Bryan Reimer
    <br>
    [Under Review at BMVC 2020]
  </p>
  <p>
    <b>MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in
    Real-World Naturalistic Driving Scenarios</b>
    <br>
    <b>Li Ding</b>, Michael Glazer, Meng Wang, Bruce Mehler, Bryan Reimer, and
    Lex Fridman
    <br>
    [<b>IEEE IV 2020</b>: NDDA Workshop (oral presentation)]
  </p>
  <p>
    <b>Value of Temporal Dynamics Information in Driving Scene Segmentation</b>
    <br>
    <b>Li Ding</b>, Jack Terwilliger, Rini Sherony, Bryan Reimer, and Lex Fridman
    <br>
    [Under Review at IEEE Trans-IV]
    [<a href="materials/1904.00758.pdf" target="_blank">pdf</a>]
    [<a href="https://arxiv.org/abs/1904.00758" target="_blank">arXiv</a>]
  </p>
  

  <hr>
  <h5>
    <b>Black Betty: MIT Human-Centered Autonomous Vehicle</b>
  </h5>
  <p style="color:dimgray">
    Research Project at MIT<br>
    Supported by Toyota and Veoneer<br>
    2018 - 2019<br>
  </p>

  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <img src="images/proj_betty.jpg" style="width:95%">
    </div>
    <div class="six columns">
      <p>
        The interaction between human and machine with growing intelligence
        challenges our assumptions about the limitations of human beings at
        their worst and the capabilities of AI systems at their best. We explore
        human-centered autonomous vehicle as an illustrative case study of
        concepts in shared autonomy. The project involves research on human
        machine collaboration, computer vision, and perception/control systems
        for semi-autonomous driving.
      </p>
    </div>
  </div>
  <br>

  <p style="color:dimgray; font-size:125%;">
    <b>References</b>
  </p>

  <p>
    <b>Project Page:</b> <a href="https://hcai.mit.edu/hcav/" target="_blank">hcai.mit.edu/hcav</a>
  </p>

  <p>
    <b>Arguing Machines: Human Supervision of Black Box AI Systems that Make Life-Critical Decisions</b>
    <br>
    Lex Fridman, <b>Li Ding</b>, Benedikt Jenik, Bryan Reimer
    <br>
    [<b>CVPR 2019</b>: Workshop on Autonomous Driving]
    [<a href="materials/fridman2019arguing.pdf" target="_blank">pdf</a>]
    [<a href="https://arxiv.org/abs/1710.04459" target="_blank">arXiv</a>]
  </p>

  <p>
    <b>Object as Distribution</b>
    <br>
    <b>Li Ding</b>, Lex Fridman
    <br>
    [Technical Report]
    [<a href="materials/1907.12929.pdf" target="_blank">pdf</a>]
    [<a href="https://arxiv.org/abs/1907.12929" target="_blank">arXiv</a>]
  </p>


  <hr>
  <h5>
    <b>Human Action Recognition in Untrimmed Videos</b>
  </h5>
  <p style="color:dimgray">
    Research Project at the University of Rochester<br>
    Supported by NSF BIGDATA<br>
    2017 - 2018<br>
  </p>

  <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
    <div class="six columns">
      <img src="images/proj_act.jpg" style="width:95%">
    </div>
    <div class="six columns">
      <p>
        One of the major challenges in video understanding is to localize and
        classify human actions in long, untrimmed videos. We proposed a new
        temporal model for action recognition and a novel iterative alignment
        approach to address the weakly supervised action localization task. The
        model was able to learn only from the transcript (ordering of actions)
        during training, and predict the exact timing of each action.
      </p>
    </div>
  </div>
  <br>

  <p style="color:dimgray; font-size:125%;">
    <b>References</b>
  </p>

  <p>
    <b>Weakly-Supervised Action Segmentation with Iterative Soft Boundary
      Assignment</b>
    <br>
    <b>Li Ding</b>, Chenliang Xu
    <br>
    [<b>CVPR 2018</b>]
    [<a href="materials/ding2018weakly.pdf" target="_blank">pdf</a>]
    [<a href="https://arxiv.org/abs/1803.10699" target="_blank">arXiv</a>]
    [<a href="https://github.com/Zephyr-D/TCFPN-ISBA" target="_blank">code</a>]
    [<a href="materials/ding2018weakly_poster.pdf" target="_blank">poster</a>]
  </p>

  <p>
    <b>TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video
      Action Segmentation</b>
    <br>
    <b>Li Ding</b>, Chenliang Xu
    <br>
    [Technical Report]
    [<a href="materials/1705.07818.pdf" target="_blank">pdf</a>]
    [<a href="https://arxiv.org/abs/1705.07818" target="_blank">arXiv</a>]
  </p>
</div>

  <!-- Footer  
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <footer>
    <div class="container">
      <hr>
      <p align="center" style="margin-top: -3rem; margin-bottom: 1rem;">
        <small>Copyright © 2017-2020 Ding, Li</small>
      </p>
    </div>
  </footer>

  <!-- End Document
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>