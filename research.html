<!-- 
  Author: Li Ding
  2020/01/23
-->

<!DOCTYPE html>
<html style="height: 100%;">

<head>
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Li Ding | 丁立</title>
  <meta name="description" content="Li Ding's personal website.">
  <meta name="author" content="Li Ding">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Raleway:400,300,600">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.7.2/css/all.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="images/favicon.png">-->
</head>


<body>
  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <header class="container">
    <br>
    <nav class="navbar">
      <ul class="navbar-list">
        <li class="navbar-item">
          <a class="navbar-link" href=".">Li Ding | 丁立</a>
        </li>
        <li class="navbar-item">
          <a class="navbar-link active" href="research.html">Research</a>
        </li>
        <li class="navbar-item">
          <a class="navbar-link" href="publications.html">Publications</a>
        </li>
        <li class="navbar-item">
          <a class="navbar-link" href="resume.html">Résumé</a>
        </li>
        <li class="navbar-item">
          <a class="navbar-link" href="misc.html">Misc.</a>
        </li>
      </ul>
    </nav>
  </header>

  <div class="container">
    <hr>




    <h5>
      <b>Neural Evolution and Selection</b>
    </h5>
    <p style="color:dimgray">
      Research Project at UMass Amherst<br>
      <!-- Supported by <br> -->
      2020 - present<br>
    </p>

    <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
      <div class="six columns">
        <img src="images/pub_neuro.png" style="width:95%">
      </div>
      <div class="six columns">
        <p>
          Modern neural networks usually have deep architectures with millions of
          parameters and connections. To make these models work well, extensive
          manual efforts in design and tuning are often required. Our work focus
          on exploring automated evolution of deep neural networks by extending
          the ideas of selection in genetic algorithms.
        </p>
      </div>
    </div>
    <br>

    <p style="color:dimgray; font-size:125%;">
      <b>References</b>
    </p>

    <p style="margin-top: 1.2em;">
      <b>Evolving Neural Selection with Adaptive Regularization</b>
      <br>
      <b>Li Ding</b> and Lee Spector
      <br>
      [<b>GECCO 2021</b>: Workshop on NeuroEvolution at Work]
      [<a href="https://dl.acm.org/doi/abs/10.1145/3449726.3463189" target="_blank">paper</a>]
      [<a href="https://www.youtube.com/watch?v=m4vgPrzOR_c" target="_blank">presentation</a>]
    </p>



    <hr>




    <h5>
      <b>Cognitive Load Estimation</b>
    </h5>
    <p style="color:dimgray">
      Research Project at MIT<br>
      Supported by Veoneer and MIT AHEAD Consortium<br>
      2018 - present<br>
    </p>

    <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
      <div class="six columns">
        <img src="images/proj_cog.jpg" style="width:95%">
      </div>
      <div class="six columns">
        <p>
          How do we measure the state of the human mind? Cognitive load has been
          shown to be an important variable for understanding human performance on
          a variety of tasks including public speaking, education, machine
          operation, and driving. Our research focuses on vision-based non-contact
          cognitive load estimation using machine learning approaches on visual
          features including pupil dynamics, glance behaviors, and facial
          expressions.
        </p>
      </div>
    </div>
    <br>

    <p style="color:dimgray; font-size:125%;">
      <b>References</b>
    </p>

    <p style="margin-top: 1.2em;">
      <b>Joint Cognitive Load Estimation and Eye Landmarks Detection in the Wild</b>
      <br>
      <b>Li Ding</b>, Jack Terwilliger, Aishni Parab, Meng Wang, Bruce Mehler, Bryan Reimer, and Lex Fridman
      <br>
      [Under Review]
    </p>

    <p style="margin-top: 1.2em;">
      <b> Real-time Cognitive Load Estimation with Multi-source Features and Individual Normalization</b>
      <br>
      Meng Wang, <b>Li Ding</b>, Pnina Gershon, Bruce Mehler, and Bryan Reimer
      <br>
      [Under Review]
    </p>

    <hr>

    <h5>
      <b>Driving Scene Perception and Edge Case Enumeration</b>
    </h5>
    <p style="color:dimgray">
      Research Project at MIT<br>
      Supported by Toyota<br>
      2017 - 2020<br>
    </p>

    <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
      <div class="six columns">
        <img src="images/proj_seg.jpg" style="width:95%">
      </div>
      <div class="six columns">
        <p>
          Solving the driving scene perception problem for driver-assistance
          systems and autonomous vehicles requires accurate and robust model
          performance in various driving scenarios, including those
          rarely-occurring edge cases. Aiming at developing a real-time perception
          system prototype, our research involves novel methods for video scene
          segmentation, large-scale driving data collection, semi-automated
          annotation, and edge case enumeration.
        </p>
      </div>
    </div>
    <br>

    <p style="color:dimgray; font-size:125%;">
      <b>References</b>
    </p>

    <p style="margin-top: 1.2em;">
      <b>Perceptual Evaluation of Driving Scene Segmentation</b>
      <br>
      <b>Li Ding</b>, Rini Sherony, Bruce Mehler, and Bryan Reimer
      <br>
      [<b>IEEE IV 2021</b>]
    </p>
    <p>
      <b>MIT-AVT Clustered Driving Scene Dataset: Evaluating Perception Systems in
        Real-World Naturalistic Driving Scenarios</b>
      <br>
      <b>Li Ding</b>, Michael Glazer, Meng Wang, Bruce Mehler, Bryan Reimer, and
      Lex Fridman
      <br>
      [<b>IEEE IV 2020</b>: NDDA Workshop (oral presentation)]
      [<a href="https://ieeexplore.ieee.org/abstract/document/9304677" target="_blank">paper</a>]
    </p>
    <p>
      <b>Value of Temporal Dynamics Information in Driving Scene Segmentation</b>
      <br>
      <b>Li Ding</b>, Jack Terwilliger, Rini Sherony, Bryan Reimer, and Lex Fridman
      <br>
      [<b>IEEE Transactions on Intelligent Vehicles</b>]
      [<a href="https://ieeexplore.ieee.org/abstract/document/9478305" target="_blank">paper</a>]
      [<a href="https://arxiv.org/abs/1904.00758" target="_blank">arXiv</a>]
      [<a href="https://ieee-dataport.org/open-access/mit-driveseg-manual-dataset" target="_blank">dataset</a>]
    </p>


    <hr>
    <h5>
      <b>Black Betty: MIT Human-Centered Autonomous Vehicle</b>
    </h5>
    <p style="color:dimgray">
      Research Project at MIT<br>
      Supported by Toyota and Veoneer<br>
      2018 - 2019<br>
    </p>

    <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
      <div class="six columns">
        <img src="images/proj_betty.jpg" style="width:95%">
      </div>
      <div class="six columns">
        <p>
          The interaction between human and machine with growing intelligence
          challenges our assumptions about the limitations of human beings at
          their worst and the capabilities of AI systems at their best. We explore
          human-centered autonomous vehicle as an illustrative case study of
          concepts in shared autonomy. The project involves research on human
          machine collaboration, computer vision, and perception/control systems
          for semi-autonomous driving.
        </p>
      </div>
    </div>
    <br>

    <p style="color:dimgray; font-size:125%;">
      <b>References</b>
    </p>

    <p>
      <b>Project Page:</b> <a href="https://hcai.mit.edu/hcav/" target="_blank">hcai.mit.edu/hcav</a>
    </p>

    <p>
      <b>Arguing Machines: Human Supervision of Black Box AI Systems that Make Life-Critical Decisions</b>
      <br>
      Lex Fridman, <b>Li Ding</b>, Benedikt Jenik, Bryan Reimer
      <br>
      [<b>CVPR 2019</b>: Workshop on Autonomous Driving]
      [<a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/WAD/Fridman_Arguing_Machines_Human_Supervision_of_Black_Box_AI_Systems_That_CVPRW_2019_paper.html" target="_blank">paper</a>]
      [<a href="https://arxiv.org/abs/1710.04459" target="_blank">arXiv</a>]
    </p>

    <hr>
    <h5>
      <b>Human Action Recognition in Untrimmed Videos</b>
    </h5>
    <p style="color:dimgray">
      Research Project at the University of Rochester<br>
      Supported by NSF BIGDATA<br>
      2017 - 2018<br>
    </p>

    <div class="row" style="display: flex; flex-wrap: wrap; align-items: center">
      <div class="six columns">
        <img src="images/proj_act.jpg" style="width:95%">
      </div>
      <div class="six columns">
        <p>
          One of the major challenges in video understanding is to localize and
          classify human actions in long, untrimmed videos. We proposed a new
          temporal model for action recognition and a novel iterative alignment
          approach to address the weakly supervised action localization task. The
          model was able to learn only from the transcript (ordering of actions)
          during training, and predict the exact timing of each action.
        </p>
      </div>
    </div>
    <br>

    <p style="color:dimgray; font-size:125%;">
      <b>References</b>
    </p>

    <p>
      <b>Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</b>
      <br>
      <b>Li Ding</b>, Chenliang Xu<br>
      [<b>CVPR 2018</b>]
      [<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Ding_Weakly-Supervised_Action_Segmentation_CVPR_2018_paper.html" target="_blank">paper</a>]
      [<a href="https://arxiv.org/abs/1803.10699" target="_blank">arXiv</a>]
      [<a href="materials/ding2018weakly_poster.pdf" target="_blank">poster</a>]
      [<a href="https://github.com/Zephyr-D/TCFPN-ISBA" target="_blank">code</a>]
    </p>

    <p>
      <b>TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video
        Action Segmentation</b>
      <br>
      <b>Li Ding</b>, Chenliang Xu
      <br>
      [Technical Report]
      [<a href="https://arxiv.org/abs/1705.07818" target="_blank">arXiv</a>]
    </p>
  </div>

  <!-- Footer  
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <footer>
    <div class="container">
      <hr>
      <p align="center" style="margin-top: -3rem; margin-bottom: 1rem;">
        <small>Copyright © 2017-2021 Ding, Li</small>
      </p>
    </div>
  </footer>

  <!-- End Document
–––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>